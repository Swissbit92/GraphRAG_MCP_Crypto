################################################################################
# GraphRAG MCP Project ‚Äî Requirements
# ------------------------------------------------------------------------------
# This environment powers:
#   ‚Ä¢ The PDF ‚Üí Chunk ‚Üí Label ‚Üí Cluster ‚Üí GraphDB ‚Üí Chroma pipeline
#   ‚Ä¢ The FastMCP servers (rag + kg)
#   ‚Ä¢ Local LLM & embedding workflows using Ollama
#
# Environment: Windows 11 / Python 3.11 / local GraphDB Desktop / Ollama
################################################################################


# ============================================================================
# üß©  CORE LIBRARIES
# ============================================================================
pymupdf                   # PDF ingestion and text extraction (used in pdf_reader.py)
tqdm                      # Progress bars across pipeline steps
python-dotenv             # Load .env configuration (OLLAMA_, GRAPHDB_, CHROMA_ vars)
numpy                     # Numeric processing, vector ops, embeddings
pandas                    # Tabular and JSONL label handling
scikit-learn              # Clustering / community detection (simple_communities.py)
networkx                  # Graph utilities for co-mention and cluster graphs
requests                  # HTTP access (Ollama API, GraphDB SPARQL, etc.)
pydantic                  # Data validation for all FastMCP I/O schemas
pytest                    # Test runner for local + CI smoke tests

# ============================================================================
# üìÑ  YAML / STRING UTILITIES
# ============================================================================
PyYAML                    # For YAML parsing / config interoperability
rapidfuzz                 # High-speed fuzzy string matching for entity cleanup

# ============================================================================
# üß†  LANGCHAIN + OLLAMA (Local LLM Inference)
# ============================================================================
# Note: leave unpinned to simplify upgrades; align all three if pinning later.
langchain                 # High-level orchestration of LLM + retriever pipelines
langchain-core            # Core abstractions (prompt templates, messages, etc.)
langchain-ollama          # Ollama model integration (used in llm_chunk_tagger.py)

# ============================================================================
# üß©  KNOWLEDGE GRAPH + RDF SUPPORT
# ============================================================================
langchain-community       # Additional LangChain integrations (Chroma, etc.)
rdflib                    # RDF serialization / parsing for KG interoperability

# ============================================================================
# üíæ  VECTOR STORE (RAG BACKEND)
# ============================================================================
chromadb                  # Persistent vector database for embeddings / retrieval

# ============================================================================
# ‚öôÔ∏è  MCP FRAMEWORK
# ============================================================================
fastmcp                   # FastMCP 2.x server framework for rag_server.py / kg_server.py


################################################################################
# Notes
# ------------------------------------------------------------------------------
# 1. No OpenAI or cloud dependencies ‚Äî fully local (Ollama backend).
# 2. GraphDB Desktop 11+ expected at http://localhost:7200 (RDFS+ / SHACL enabled).
# 3. Ollama models (e.g., llama3.1:latest, nomic-embed-text) pulled locally.
# 4. To ensure reproducibility, pin the langchain trio to the same minor if upgrading.
################################################################################
