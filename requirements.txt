################################################################################
# GraphRAG MCP Project â€” Requirements
# ------------------------------------------------------------------------------
# This environment powers:
#   â€¢ The PDF â†’ Chunk â†’ Label â†’ Cluster â†’ GraphDB â†’ Chroma pipeline
#   â€¢ The FastMCP servers (rag + kg)
#   â€¢ Local LLM & embedding workflows using Ollama
#
# Environment: Windows 11 / Python 3.11 / local GraphDB Desktop / Ollama
################################################################################


# ============================================================================
# ðŸ§©  CORE LIBRARIES
# ============================================================================
pymupdf==1.24.9           # PDF ingestion and text extraction (used in pdf_reader.py)
tqdm==4.66.4              # Progress bars across pipeline steps
python-dotenv==1.0.1      # Load .env configuration (OLLAMA_, GRAPHDB_, CHROMA_ vars)
numpy==1.26.4             # Numeric processing, vector ops, embeddings
pandas==2.2.2             # Tabular and JSONL label handling
scikit-learn==1.5.1       # Clustering / community detection (simple_communities.py)
networkx==3.3             # Graph utilities for co-mention and cluster graphs
requests==2.32.3          # HTTP access (Ollama API, GraphDB SPARQL, etc.)
pydantic==2.8.2           # Data validation for all FastMCP I/O schemas
pytest==7.4.2             # Test runner for local + CI smoke tests

# ============================================================================
# ðŸ“„  YAML / STRING UTILITIES
# ============================================================================
PyYAML==6.0.2             # For YAML parsing / config interoperability
rapidfuzz==3.9.6          # High-speed fuzzy string matching for entity cleanup

# ============================================================================
# ðŸ§   LANGCHAIN + OLLAMA (Local LLM Inference)
# ============================================================================
# Note: leave unpinned to simplify upgrades; align all three if pinning later.
langchain                 # High-level orchestration of LLM + retriever pipelines
langchain-core            # Core abstractions (prompt templates, messages, etc.)
langchain-ollama          # Ollama model integration (used in llm_chunk_tagger.py)

# ============================================================================
# ðŸ§©  KNOWLEDGE GRAPH + RDF SUPPORT
# ============================================================================
langchain-community==0.3.4  # Additional LangChain integrations (Chroma, etc.)
rdflib==7.1.1               # RDF serialization / parsing for KG interoperability

# ============================================================================
# ðŸ’¾  VECTOR STORE (RAG BACKEND)
# ============================================================================
chromadb>=0.4.24          # Persistent vector database for embeddings / retrieval

# ============================================================================
# âš™ï¸  MCP FRAMEWORK
# ============================================================================
fastmcp>=2.2              # FastMCP 2.x server framework for rag_server.py / kg_server.py


################################################################################
# Notes
# ------------------------------------------------------------------------------
# 1. No OpenAI or cloud dependencies â€” fully local (Ollama backend).
# 2. GraphDB Desktop 11+ expected at http://localhost:7200 (RDFS+ / SHACL enabled).
# 3. Ollama models (e.g., llama3.1:latest, nomic-embed-text) pulled locally.
# 4. To ensure reproducibility, pin the langchain trio to the same minor if upgrading.
################################################################################
