import os
import json
import hashlib
from pathlib import Path
from typing import Iterable, Dict, Any, List, Optional, Generator

import requests
import chromadb
from chromadb.config import Settings
from chromadb.api.types import Documents, Embeddings  # for type compatibility
from chromadb.utils import embedding_functions


# ----------------------------- Configuration ----------------------------------

# How many per-item entity slots to store as scalar metadata fields.
# Lists are not allowed in Chroma metadata, so we scatter across fixed slots.
MAX_ENTITY_SLOTS = int(os.getenv("RAG_MAX_ENTITY_SLOTS", "8"))


# ----------------------------- Embedding providers ----------------------------

class OllamaEmbeddingFunction:
    """
    Minimal local embedding via Ollama REST API compatible with Chroma's interface.

    Chroma (>=0.4.16) expects:
      - name(self) -> str
      - __call__(self, input: Documents) -> Embeddings

    Env:
      USE_OLLAMA_EMBED=true|false
      OLLAMA_BASE=http://127.0.0.1:11434
      OLLAMA_EMBED_MODEL=nomic-embed-text
    """

    def __init__(self, base: Optional[str] = None, model: Optional[str] = None):
        self.base = (base or os.getenv("OLLAMA_BASE", "http://127.0.0.1:11434")).rstrip("/")
        self.model = model or os.getenv("OLLAMA_EMBED_MODEL", "nomic-embed-text")
        self.session = requests.Session()

    def name(self) -> str:
        return f"ollama:{self.model}"

    def __call__(self, input: Documents) -> Embeddings:
        # input is list[str] per Chroma types (or a single str)
        if isinstance(input, str):
            texts = [input]
        else:
            texts = list(input or [])

        url = f"{self.base}/api/embeddings"
        out: List[List[float]] = []
        for t in texts:
            resp = self.session.post(url, json={"model": self.model, "prompt": t})
            resp.raise_for_status()
            data = resp.json()
            vec = data.get("embedding")
            if not isinstance(vec, list):
                raise RuntimeError(f"Ollama embedding failed: {data}")
            out.append(vec)
        return out


class SentenceTransformerWrapper:
    """
    Wrap sentence-transformers to match Chroma's embedding function interface.
    """

    def __init__(self, model_name: str):
        self._ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)
        self._name = f"sentence-transformers:{model_name}"

    def name(self) -> str:
        return self._name

    def __call__(self, input: Documents) -> Embeddings:
        if isinstance(input, str):
            docs = [input]
        else:
            docs = list(input or [])
        return self._ef(docs)


def build_embedding_function():
    """
    Priority: OLLAMA -> sentence-transformers -> None (caller must pass embeddings).
    """
    use_ollama = os.getenv("USE_OLLAMA_EMBED", "true").lower() in ("1", "true", "yes")
    if use_ollama:
        return OllamaEmbeddingFunction()

    model_name = os.getenv("SENTENCE_TRANSFORMER_MODEL")
    if model_name:
        return SentenceTransformerWrapper(model_name=model_name)

    # No embedding function configured; Chroma will require explicit embeddings on upsert
    return None


# ----------------------------- RAG store --------------------------------------

class ChromaRAG:
    def __init__(
        self,
        persist_dir: Optional[str] = None,
        collection: str = "whitepapers",
        embedding_fn=None,
    ):
        persist_dir = persist_dir or os.getenv("CHROMA_DIR", ".chroma")
        self.client = chromadb.PersistentClient(
            path=persist_dir,
            settings=Settings(allow_reset=False)
        )

        self.embedding_fn = embedding_fn if embedding_fn is not None else build_embedding_function()

        # IMPORTANT: pass embedding_function to avoid conflicts and let Chroma build vectors
        self.col = self.client.get_or_create_collection(
            name=collection,
            embedding_function=self.embedding_fn,
            metadata={"hnsw:space": "cosine"},
        )

    # -- helpers ----------------------------------------------------------------
    @staticmethod
    def _normalize_id(s: str) -> str:
        return hashlib.sha1(s.encode("utf-8")).hexdigest()

    # -- API --------------------------------------------------------------------
    def upsert_chunks(
        self,
        items: Iterable[Dict[str, Any]],
        id_prefix: str = "chunk:",
        require_embeddings: bool = False,
        batch_size: int = 256,
    ):
        """
        items: iterable of dicts with keys:
            - id (optional) | autogenerated from (doc_id, chunk_id, sha1) if absent
            - text (required)
            - metadata: { doc_id, chunk_id, entity_ids: [full IRIs], section_type, page, sha1, embed_model }
        Notes:
            - We convert entity_ids list into scalar slots: entity_id_0..entity_id_{N-1} and a CSV string entity_ids_csv.
        """
        batch_ids: List[str] = []
        batch_docs: List[str] = []
        batch_mds: List[Dict[str, Any]] = []

        def _scalarize_metadata(md: Dict[str, Any]) -> Dict[str, Any]:
            md = dict(md or {})
            # Convert entity_ids (list) to scalar fields
            ids_list = md.pop("entity_ids", None) or []
            # keep a compact string for inspection / debug
            md["entity_ids_csv"] = "|".join(ids_list) if ids_list else ""
            md["entity_count"] = int(len(ids_list))
            # spread across fixed slots
            for i in range(min(len(ids_list), MAX_ENTITY_SLOTS)):
                md[f"entity_id_{i}"] = ids_list[i]
            return md

        def flush():
            if not batch_ids:
                return
            self.col.upsert(ids=batch_ids, documents=batch_docs, metadatas=batch_mds)
            batch_ids.clear()
            batch_docs.clear()
            batch_mds.clear()

        for it in items:
            text = (it.get("text") or "").strip()
            if not text:
                continue
            md_raw  = it.get("metadata", {}) or {}
            md = _scalarize_metadata(md_raw)

            cid = it.get("id") or self._normalize_id(f"{md.get('doc_id','')}/{md.get('chunk_id','')}/{md.get('sha1','')}")
            batch_ids.append(f"{id_prefix}{cid}")
            batch_docs.append(text)
            batch_mds.append(md)

            if len(batch_ids) >= batch_size:
                flush()
        flush()

    def query(
        self,
        text: Optional[str] = None,
        entity_ids: Optional[List[str]] = None,
        k: int = 8,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Basic retrieval. If entity_ids provided, filter by OR over entity_id_0..entity_id_{N-1}.
        """
        where = dict(where or {})
        if entity_ids:
            # Build OR over all slots for provided entity_ids
            or_clauses: List[Dict[str, Any]] = []
            for i in range(MAX_ENTITY_SLOTS):
                or_clauses.append({f"entity_id_{i}": {"$in": entity_ids}})
            # If there's already an $or, extend it; else add fresh
            existing_or = where.get("$or")
            if isinstance(existing_or, list):
                existing_or.extend(or_clauses)
            else:
                where["$or"] = or_clauses

        res = self.col.query(
            query_texts=[text] if text else None,
            n_results=k,
            where=where if where else None,
        )
        return res


# ---------------- convenience loader for your pipeline outputs ----------------

def iter_pipeline_records(
    labels_dir: Path,
    chunks_dir: Path,
) -> Generator[Dict[str, Any], None, None]:
    """
    Joins labels (JSONL) with chunk texts by (doc_id, chunk_id).

    Expects:
      labels_dir/*.labels.jsonl
      chunks_dir/*.chunks.jsonl   -> records with keys: doc_id, chunk_id, text, page?, sha1?
    """
    # Build a small in-memory index of chunk texts for the current file batch
    chunk_index: Dict[tuple, Dict[str, Any]] = {}

    for chunk_file in sorted(chunks_dir.glob("*.chunks.jsonl")):
        with chunk_file.open("r", encoding="utf-8") as f:
            for line in f:
                try:
                    rec = json.loads(line)
                except Exception:
                    continue
                key = (rec.get("doc_id"), rec.get("chunk_id"))
                if key[0] and key[1]:
                    chunk_index[key] = rec

    for lab_file in sorted(labels_dir.glob("*.labels.jsonl")):
        with lab_file.open("r", encoding="utf-8") as f:
            for line in f:
                try:
                    lab = json.loads(line)
                except Exception:
                    continue
                key = (lab.get("doc_id"), lab.get("chunk_id"))
                chunk = chunk_index.get(key) or {}
                text = chunk.get("text", "")
                if not text:
                    continue

                # Resolve entity IRIs (full KG IRIs)
                entity_ids: List[str] = []
                ents = lab.get("entities", {}) or {}
                # local import to avoid top-level circular imports
                from src.kg.namespaces import iri_entity
                for kind, vals in ents.items():
                    for v in vals or []:
                        v_clean = (v or "").strip()
                        if not v_clean:
                            continue
                        entity_ids.append(iri_entity(kind, v_clean)[1:-1])  # strip < >

                # Prefer first section_type string if list
                section_type_val = None
                st = lab.get("section_type")
                if isinstance(st, list) and st:
                    section_type_val = st[0]
                elif isinstance(st, str):
                    section_type_val = st

                yield {
                    "id": None,  # auto from hash
                    "text": text,
                    "metadata": {
                        "doc_id": lab.get("doc_id"),
                        "chunk_id": lab.get("chunk_id"),
                        # entity_ids list will be scalarized in upsert()
                        "entity_ids": entity_ids,
                        "section_type": section_type_val,
                        "page": chunk.get("page"),
                        "sha1": chunk.get("sha1"),
                        "embed_model": os.getenv("EMBED_MODEL_NAME", os.getenv("OLLAMA_EMBED_MODEL", "nomic-embed-text")),
                    },
                }
