# ğŸ§  GraphRAG MCP  
> **Entity-centric Retrieval-Augmented Generation for Crypto Whitepapers**  
> _Local-first â€¢ Private â€¢ FastMCP-ready_

<p align="left">
  <img alt="Python" src="https://img.shields.io/badge/Python-3.11+-3776AB?logo=python&logoColor=white" />
  <img alt="Ollama" src="https://img.shields.io/badge/Ollama-Local%20LLMs-000000?logo=ollama&logoColor=white" />
  <img alt="ChromaDB" src="https://img.shields.io/badge/Chroma-Vector%20Store-241F31" />
  <img alt="GraphDB" src="https://img.shields.io/badge/GraphDB-Ontotext-FF5A1F" />
  <img alt="FastMCP" src="https://img.shields.io/badge/FastMCP-2.x-4F46E5" />
  <img alt="LangChain" src="https://img.shields.io/badge/LangChain-Local%20Orchestration-00BFFF" />
  <img alt="License" src="https://img.shields.io/badge/Privacy-Local%20Only-16a34a" />
</p>

---

## 1) âœ¨ Overview

**GraphRAG MCP** is a modular, **local-first** system that turns crypto whitepapers into an **entity-centric Knowledge Graph** and a **vector-searchable corpus**, then answers questions with **RAG + optional KG enrichment + LLM synthesis** â€” all via standardized **FastMCP** tools.

### Why this project?
- ğŸ›¡ï¸ **Privacy by default:** runs entirely on your machine (Ollama, Chroma, GraphDB).
- âš¡ **Fast & focused:** entity-filtered retrieval narrows context to the right tokens/protocols.
- ğŸ§© **Composable:** exposes `rag.*` and `kg.*` tools so an **MCP Coordinator** or **Streamlit** app can orchestrate multi-tool workflows.
- ğŸ§  **Explainable answers:** returns **citations with doc/chunk/entity IDs** for every response.

---

### ğŸ” End-to-End Flow

**Core data flow:**
PDF â†’ semantic_splitter â†’ llm_chunk_tagger â†’ postprocess
â†’ Chroma (vector store) + GraphDB (entity KG)
â†’ FastMCP: rag_server + kg_server

**Typical usage:**
1. Ingest and label whitepapers â†’ build embeddings and insert entities.  
2. Ask questions via `rag.qa` (semantic + entity-filtered retrieval), optionally enrich with KG labels/aliases.  
3. Get concise LLM answers with inline citations to source chunks.

---

## 2ï¸âƒ£ Features

### ğŸ§© Knowledge Graph (KG)
- **Entity-only architecture** using RDF/OWL ontologies (`mcp-core.ttl`, `mcp-crypto.ttl`).
- Built on **Ontotext GraphDB 11+** with SHACL validation and SPARQL/GraphQL endpoints.
- Stores canonical entities such as tokens, protocols, components, and organizations.
- Enables KG enrichment for RAG answers via aliases, labels, and relationships.

### ğŸ” Vector Retrieval (RAG)
- **ChromaDB** acts as the persistent vector store for chunk embeddings.
- Embeddings generated using **Ollamaâ€™s** `nomic-embed-text` model.
- Supports **semantic** and **entity-filtered** retrieval modes for accurate context fetching.
- Each chunk contains structured metadata: `doc_id`, `chunk_id`, `entity_ids`, `section_type`, and `page`.

### ğŸ§  Local LLM Inference
- Uses **Ollama** for fully local inference â€” no external API keys required.
- Compatible with models like `llama3.1:latest`, `qwen2.5:14b-instruct`, or `mistral`.
- Performs labeling, summarization, and final QA synthesis.
- Includes deterministic **mock mode** for offline testing and CI.

### âš™ï¸ FastMCP Servers
- Two modular servers expose tools via **FastMCP 2.x**:
  - `rag` â†’ `rag.search`, `rag.embed_and_index`, `rag.reindex`, `rag.delete`, `rag.health`, `rag.qa`
  - `kg` â†’ `sparql_query`, `sparql_update`, `push_labels`, `validate_labels`, `list_documents`, `kg.health`
- Both run locally via stdio and are MCP-Coordinator compatible.

### ğŸ”’ Privacy & Portability
- 100% offline operation â€” suitable for air-gapped or research environments.
- Reproducible local stack (GraphDB + Chroma + Ollama + FastMCP).
- Works seamlessly on Windows 11, macOS, or Linux.

### ğŸš€ Integration Ready
- Plug-and-play with **MCP Coordinators** or **Streamlit apps** for end-user Q&A.
- Can interoperate with other MCPs such as:
  - Brave API MCP (web search)
  - MongoDB MCP (strategy data)
  - Telegram MCP (messaging)
  - Gmail MCP (email retrieval)
- Returns clean JSON outputs for easy chaining into agentic workflows.

---

## 3ï¸âƒ£ ğŸ—ï¸ Architecture

The **GraphRAG MCP** architecture combines **Knowledge Graph reasoning**, **Vector-based retrieval**, and **Local LLM synthesis** â€” all under the **MCP** interoperability standard.  
Itâ€™s designed for *clarity*, *privacy*, and *modular scalability*.

---

### ğŸ§­ High-Level Overview

| Layer | Technology | Purpose | Example Components |
|:------|:------------|:---------|:--------------------|
| ğŸ—‚ **Ingestion Layer** | Python + LangChain | Reads PDFs, splits into semantic chunks, labels with LLMs | `pdf_reader.py`, `semantic_splitter.py`, `llm_chunk_tagger.py` |
| ğŸ§© **Knowledge Graph Layer (KG)** | GraphDB (Ontotext) + RDFLib | Stores canonical entities (tokens, protocols, organizations) | `graphdb_sink.py`, `namespaces.py`, SHACL shapes |
| ğŸ’¾ **Vector Retrieval Layer (RAG)** | ChromaDB + Ollama embeddings | Stores text chunks + metadata + embeddings for semantic retrieval | `chroma_store.py`, `.chroma/` |
| âš™ï¸ **MCP Layer** | FastMCP 2.x | Exposes standardized MCP tools (`rag.*`, `kg.*`) | `rag_server.py`, `kg_server.py` |
| ğŸ§  **LLM Synthesis Layer** | Ollama LLMs (`llama3.1`, `qwen2.5`) | Answers questions with retrieved context + KG enrichment | `rag.qa`, `llm_chunk_tagger` |
| ğŸ’¬ **User Interface Layer** | MCP Coordinator / Streamlit | Connects multiple MCPs for conversational Q&A | Coordinator UI or custom Streamlit dashboard |

---

### ğŸ”¹ Data Flow Diagram

```text
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              Whitepapers               â”‚
            â”‚ (PDFs, research papers, documentation) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚      ğŸ“„ Ingestion & Labeling                â”‚
            â”‚  pdf_reader â†’ semantic_splitter â†’           â”‚
            â”‚  llm_chunk_tagger â†’ postprocess             â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                 â”‚
                        â–¼                 â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ ğŸ§  GraphDB KG  â”‚    â”‚ ğŸ’¾ Chroma RAG      â”‚
            â”‚ Entities & IRIs â”‚   â”‚ Chunks + Embeddings â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                     â”‚
                     â–¼                     â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ âš™ï¸ kg_server  â”‚     â”‚ âš™ï¸ rag_server â”‚
               â”‚ (FastMCP)     â”‚      â”‚ (FastMCP)     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ ğŸ’¬ MCP Coordinator / Streamlit â”‚
                  â”‚  User-facing Q&A Interface     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---

### ğŸ§  How It Works (Step-by-Step)

| Step | Description | Input | Output |
|:----:|:-------------|:------|:--------|
| **1ï¸âƒ£** | **PDF Parsing** | Whitepaper PDF | Raw text pages |
| **2ï¸âƒ£** | **Semantic Splitting** | Raw text | Meaningful chunks (by section/topic) |
| **3ï¸âƒ£** | **LLM Labeling** | Chunk text | Entities, relations, and section labels |
| **4ï¸âƒ£** | **Postprocessing** | Labeled chunks | Cleaned JSONL with canonical entity IRIs |
| **5ï¸âƒ£** | **Indexing** | JSONL labels | Chroma embeddings + KG triples |
| **6ï¸âƒ£** | **Retrieval (rag.search)** | Query text / entities | Relevant chunks |
| **7ï¸âƒ£** | **Enrichment (optional)** | Retrieved entities | KG aliases, definitions |
| **8ï¸âƒ£** | **Answer Synthesis (rag.qa)** | Question + context | Concise answer with citations |

---

### ğŸŒ Data Modalities

| Data Type | Storage | Example |
|:-----------|:--------|:--------|
| ğŸ§± **Entity** | GraphDB | `<https://kg.mcp.ai/id/token/bitcoin>` â†’ `rdf:type crypto:Token` |
| ğŸ“œ **Chunk** | Chroma | â€œBitcoin is a peer-to-peer electronic cash systemâ€¦â€ |
| ğŸ§© **Embedding** | Chroma / Ollama | 768-dim `nomic-embed-text` vector |
| ğŸ§® **Provenance** | Metadata | `doc_id`, `chunk_id`, `page`, `entity_ids[]` |
| ğŸ’¬ **Answer** | MCP JSON | `{ "answer": "...", "citations": [...] }` |

---

### ğŸ§± Core MCP Tools

| Server | Tool | Description |
|:--------|:------|:-------------|
| ğŸ§© **RAG** | `rag.search` | Semantic search over chunks |
| | `rag.embed_and_index` | Add new labeled chunks to index |
| | `rag.reindex` | Rebuild from outputs directory |
| | `rag.delete` | Delete by IDs or filters |
| | `rag.qa` | Question answering with LLM synthesis |
| | `rag.health` | Diagnostics and store info |
| ğŸ§  **KG** | `sparql_query` / `sparql_update` | Execute SPARQL against GraphDB |
| | `push_labels` / `validate_labels` | Add or validate KG entries |
| | `list_documents`, `get_chunk` | Retrieve document metadata |
| | `kg.health` | Check GraphDB repository status |

---

### ğŸ§¬ Technology Stack Summary

| Category | Technology | Purpose |
|:----------|:------------|:---------|
| **Language** | ğŸ Python 3.11 | Core pipeline and servers |
| **LLM Backend** | ğŸ§  Ollama | Local inference for labeling & QA |
| **Vector Store** | ğŸ’¾ ChromaDB | Embeddings and chunk retrieval |
| **Knowledge Graph** | ğŸ§© GraphDB | RDF-based entity storage |
| **Interoperability** | âš™ï¸ FastMCP 2.x | Exposes tools for coordinators |
| **Testing** | ğŸ§ª Pytest | Offline & integration tests |
| **Visualization / UI** | ğŸ’¬ Streamlit / MCP Coordinator | Front-end for user Q&A |

---

## 4ï¸âƒ£ âš™ï¸ Installation & Setup

Set up your local **GraphRAG MCP environment** in just a few steps!  
This stack runs fully offline and integrates seamlessly with **Ollama**, **GraphDB**, and **Chroma**.

---

### ğŸ§¾ Prerequisites

| Requirement | Description | Example |
|:-------------|:-------------|:----------|
| ğŸ **Python** | Version **3.11+** recommended | `python --version` â†’ `Python 3.11.8` |
| ğŸ§  **Ollama** | Local LLM runtime (for inference + embeddings) | `ollama pull llama3.1:latest` |
| ğŸ§© **GraphDB Desktop 11+** | Local Knowledge Graph database | runs at `http://localhost:7200` |
| ğŸ’¾ **ChromaDB** | Vector store for embeddings | auto-initialized under `.chroma/` |
| ğŸ§° **FastMCP** | Multi-Component Platform runtime (2.x) | installed via `pip` |

---

### ğŸ§± Folder Layout (simplified)

| Folder | Purpose | Example Contents |
|:--------|:----------|:----------------|
| `src/` | Core codebase | `pipeline.py`, `mcp/`, `kg/`, `rag/` |
| `outputs/run_simple/` | Generated outputs | labeled chunks, reports, embeddings |
| `.chroma/` | Chroma persistent vector store | `chroma.sqlite3`, `index/` |
| `.env` | Environment configuration | Ollama, GraphDB, Chroma settings |
| `tests/` | Offline unit tests | `test_rag_qa.py`, `test_kg_server.py` |

---

### ğŸ§° Step-by-Step Setup

#### ğŸª„ 1ï¸âƒ£ Clone & Create Virtual Environment
\`\`\`bash
git clone <your_repo_url>
cd GraphRAG_MCP
python -m venv .venv
\`\`\`

#### âš¡ 2ï¸âƒ£ Activate Environment
| OS | Command |
|:---|:---------|
| ğŸªŸ **Windows (PowerShell)** | `.venv\Scripts\activate` |
| ğŸ§ **Linux / macOS** | `source .venv/bin/activate` |

#### ğŸ“¦ 3ï¸âƒ£ Install Dependencies
\`\`\`bash
pip install -r requirements.txt
\`\`\`

#### âš™ï¸ 4ï¸âƒ£ Verify Installation
\`\`\`bash
python -m src.mcp.rag_server --list-tools
python -m src.mcp.kg_server --list-tools
\`\`\`

âœ… You should see tools like **`rag.qa`**, **`rag.search`**, and **`kg.health`**.

---

### ğŸ§  Optional: Preload Ollama Models

| Model | Purpose | Pull Command |
|:-------|:----------|:--------------|
| ğŸ¦™ **llama3.1:latest** | Default reasoning + summarization model | `ollama pull llama3.1:latest` |
| ğŸ§© **nomic-embed-text** | Embedding model for RAG vectorization | `ollama pull nomic-embed-text` |
| ğŸ¤– **qwen2.5:14b-instruct** | Larger model for complex QA tasks | `ollama pull qwen2.5:14b-instruct` |

---

### ğŸ” Quick Sanity Check

Run a quick health diagnostic to ensure everything is configured correctly:

\`\`\`bash
pytest -q
python -m src.mcp.rag_server --run-tool rag.health
python -m src.mcp.kg_server --run-tool kg.health
\`\`\`

If both return âœ… **OK**, youâ€™re ready to run the pipeline and start querying your **Knowledge Graph + RAG** system!

