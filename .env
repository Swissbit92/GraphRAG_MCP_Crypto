###############################################################################
# MCP GraphRAG Project ‚Äî Environment Variables (.env)
# Local-first setup: GraphDB + Chroma + Ollama + FastMCP
###############################################################################

# =============================================================================
# üß† OLLAMA SETTINGS ‚Äî Local LLM inference
# =============================================================================
# Model used for reasoning and synthesis (in QA and pipeline)
OLLAMA_MODEL=llama3.1:latest
# Alternative (commented for easy switch)
# OLLAMA_MODEL=qwen2.5:14b-instruct

# Ollama local endpoint
OLLAMA_BASE=http://127.0.0.1:11434

# Embedding model for RAG indexing / retrieval
USE_OLLAMA_EMBED=true
OLLAMA_EMBED_MODEL=nomic-embed-text

# Optional LLM parameters
OLLAMA_TEMPERATURE=0.2
OLLAMA_NUM_CTX=4096

# =============================================================================
# üìö RAG STORAGE ‚Äî Chroma Vector Store
# =============================================================================
# Directory for persistent Chroma index
CHROMA_DIR=.chroma

# Default collection used by RAG (can be overridden per tool call)
CHROMA_COLLECTION=whitepapers

# Default output folder for pipeline JSON/MD reports
RAG_OUTPUTS_DIR=outputs/run_simple

# =============================================================================
# üï∏Ô∏è KNOWLEDGE GRAPH ‚Äî GraphDB Repository
# =============================================================================
GRAPHDB_URL=http://localhost:7200
GRAPHDB_REPOSITORY=mcp_kg
GRAPHDB_USERNAME=admin
# GRAPHDB_PASSWORD=                # optional; add if GraphDB auth is enabled

# Push triples to GraphDB during pipeline execution
GRAPHDB_PUSH=true

# Entity-only KG mode (no Chunk/Mention nodes)
KG_ENTITY_ONLY=true

# =============================================================================
# üí¨ QUESTION ANSWERING (rag.qa)
# =============================================================================
# Override model for synthesis (defaults to OLLAMA_MODEL if not set)
QA_LLM_MODEL=llama3.1:latest

# Mode: "mock" for offline deterministic answers in tests; remove for real LLM
QA_LLM_MODE=mock

# Whether to enrich RAG retrieval with KG labels/aliases via SPARQL
QA_KG_ENRICH=true

# =============================================================================
# ‚öôÔ∏è FastMCP / Runtime Options
# =============================================================================
# These can be overridden per server or MCP Coordinator
RAG_MCP_LOG_LEVEL=INFO

# =============================================================================
# (Optional) Streamlit / UI Coordinator Configuration
# =============================================================================
# STREAMLIT_PORT=8501
# STREAMLIT_THEME=dark

###############################################################################
# End of .env
###############################################################################
# Note: To load these environment variables, run:
#    source .env
# or use a tool like 'direnv' or 'python-dotenv' in your application.