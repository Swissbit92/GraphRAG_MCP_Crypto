# ğŸ§  GraphRAG MCP  

> **Entity-centric Retrieval-Augmented Generation for Crypto Whitepapers**  
> _Local-first â€¢ Private â€¢ FastMCP-ready_

<p align="center">
  <img src="images/GRAPH_RAG_MCP.png" alt="GraphRAG MCP â€“ Eeva AI Cyberpunk Header" width="800"/>
</p>


<!-- Badges -->
![Python](https://img.shields.io/badge/Python-3.11+-3776AB?logo=python&logoColor=white) ![Ollama](https://img.shields.io/badge/Ollama-Local%20LLMs-000000?logo=ollama&logoColor=white) ![ChromaDB](https://img.shields.io/badge/Chroma-Vector%20Store-241F31) ![GraphDB](https://img.shields.io/badge/GraphDB-Ontotext-FF5A1F) ![FastMCP](https://img.shields.io/badge/FastMCP-2.x-4F46E5) ![LangChain](https://img.shields.io/badge/LangChain-Local%20Orchestration-00BFFF) ![Privacy](https://img.shields.io/badge/Privacy-Local%20Only-16a34a)

---

## 1) âœ¨ Overview

**GraphRAG MCP** is a modular, **local-first** system that turns crypto whitepapers into an **entity-centric Knowledge Graph** and a **vector-searchable corpus**, then answers questions with **RAG + optional KG enrichment + LLM synthesis** â€” all via standardized **FastMCP** tools.

### Why this project?

- ğŸ›¡ï¸ **Privacy by default:** runs entirely on your machine (Ollama, Chroma, GraphDB).
- âš¡ **Fast & focused:** entity-filtered retrieval narrows context to the right tokens/protocols.
- ğŸ§© **Composable:** exposes `rag.*` and `kg.*` tools so an **MCP Coordinator** or **Streamlit** app can orchestrate multi-tool workflows.
- ğŸ§  **Explainable answers:** returns **citations with doc/chunk/entity IDs** for every response.

---

### ğŸ” Typical usage

1. Ingest and label whitepapers â†’ build embeddings and insert entities.  
2. Ask questions via `rag.qa` (semantic + entity-filtered retrieval), optionally enrich with KG labels/aliases.  
3. Get concise LLM answers with inline citations to source chunks.

---

## 2ï¸âƒ£ Features

### ğŸ§© Knowledge Graph (KG)

- **Entity-only architecture** using RDF/OWL ontologies (`mcp-core.ttl`, `mcp-crypto.ttl`).
- Built on **Ontotext GraphDB 11+** with SHACL validation and SPARQL/GraphQL endpoints.
- Stores canonical entities such as tokens, protocols, components, and organizations.
- Enables KG enrichment for RAG answers via aliases, labels, and relationships.

### ğŸ” Vector Retrieval (RAG)

- **ChromaDB** acts as the persistent vector store for chunk embeddings.
- Embeddings generated using **Ollamaâ€™s** `nomic-embed-text` model.
- Supports **semantic** and **entity-filtered** retrieval modes for accurate context fetching.
- Each chunk contains structured metadata: `doc_id`, `chunk_id`, `entity_ids`, `section_type`, and `page`.

### ğŸ§  Local LLM Inference

- Uses **Ollama** for fully local inference â€” no external API keys required.
- Compatible with models like `llama3.1:latest`, `qwen2.5:14b-instruct`, or `mistral`.
- Performs labeling, summarization, and final QA synthesis.
- Includes deterministic **mock mode** for offline testing and CI.

### âš™ï¸ FastMCP Servers

- Two modular servers expose tools via **FastMCP 2.x**:
  - `rag` â†’ `rag.search`, `rag.embed_and_index`, `rag.reindex`, `rag.delete`, `rag.health`, `rag.qa`
  - `kg` â†’ `sparql_query`, `sparql_update`, `push_labels`, `validate_labels`, `list_documents`, `kg.health`
- Both run locally via stdio and are MCP-Coordinator compatible.

### ğŸ”’ Privacy & Portability

- 100% offline operation â€” suitable for air-gapped or research environments.
- Reproducible local stack (GraphDB + Chroma + Ollama + FastMCP).
- Works seamlessly on Windows 11, macOS, or Linux.

### ğŸš€ Integration Ready

- Plug-and-play with **MCP Coordinators** or **Streamlit apps** for end-user Q&A.
- Can interoperate with other MCPs such as:
  - Brave API MCP (web search)
  - MongoDB MCP (strategy data)
  - Telegram MCP (messaging)
  - Gmail MCP (email retrieval)
- Returns clean JSON outputs for easy chaining into agentic workflows.

---

## 3ï¸âƒ£ ğŸ—ï¸ Architecture

The **GraphRAG MCP** architecture combines **Knowledge Graph reasoning**, **Vector-based retrieval**, and **Local LLM synthesis** â€” all under the **MCP** interoperability standard.  
Itâ€™s designed for *clarity*, *privacy*, and *modular scalability*.

---

### ğŸ§­ High-Level Overview

| Layer | Technology | Purpose | Example Components |
|:------|:------------|:---------|:--------------------|
| ğŸ—‚ **Ingestion Layer** | Python + LangChain | Reads PDFs, splits into semantic chunks, labels with LLMs | `pdf_reader.py`, `semantic_splitter.py`, `llm_chunk_tagger.py` |
| ğŸ§© **Knowledge Graph Layer (KG)** | GraphDB (Ontotext) + RDFLib | Stores canonical entities (tokens, protocols, organizations) | `graphdb_sink.py`, `namespaces.py`, SHACL shapes |
| ğŸ’¾ **Vector Retrieval Layer (RAG)** | ChromaDB + Ollama embeddings | Stores text chunks + metadata + embeddings for semantic retrieval | `chroma_store.py`, `.chroma/` |
| âš™ï¸ **MCP Layer** | FastMCP 2.x | Exposes standardized MCP tools (`rag.*`, `kg.*`) | `rag_server.py`, `kg_server.py` |
| ğŸ§  **LLM Synthesis Layer** | Ollama LLMs (`llama3.1`, `qwen2.5`) | Answers questions with retrieved context + KG enrichment | `rag.qa`, `llm_chunk_tagger` |
| ğŸ’¬ **User Interface Layer** | MCP Coordinator / Streamlit | Connects multiple MCPs for conversational Q&A | Coordinator UI or custom Streamlit dashboard |

---

### ğŸ”¹ Data Flow Diagram

```text
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              Whitepapers               â”‚
            â”‚ (PDFs, research papers, documentation) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚      ğŸ“„ Ingestion & Labeling                â”‚
            â”‚  pdf_reader â†’ semantic_splitter â†’           â”‚
            â”‚  llm_chunk_tagger â†’ postprocess             â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                 â”‚
                        â–¼                 â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ ğŸ§  GraphDB KG  â”‚    â”‚ ğŸ’¾ Chroma RAG      â”‚
            â”‚ Entities & IRIs â”‚   â”‚ Chunks + Embeddings â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                     â”‚
                     â–¼                     â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ âš™ï¸ kg_server  â”‚     â”‚ âš™ï¸ rag_server â”‚
               â”‚ (FastMCP)     â”‚      â”‚ (FastMCP)     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ ğŸ’¬ MCP Coordinator / Streamlit â”‚
                  â”‚  User-facing Q&A Interface     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---

### ğŸ§  How It Works (Step-by-Step)

| Step | Description | Input | Output |
|:----:|:-------------|:------|:--------|
| **1ï¸âƒ£** | **PDF Parsing** | Whitepaper PDF | Raw text pages |
| **2ï¸âƒ£** | **Semantic Splitting** | Raw text | Meaningful chunks (by section/topic) |
| **3ï¸âƒ£** | **LLM Labeling** | Chunk text | Entities, relations, and section labels |
| **4ï¸âƒ£** | **Postprocessing** | Labeled chunks | Cleaned JSONL with canonical entity IRIs |
| **5ï¸âƒ£** | **Indexing** | JSONL labels | Chroma embeddings + KG triples |
| **6ï¸âƒ£** | **Retrieval (rag.search)** | Query text / entities | Relevant chunks |
| **7ï¸âƒ£** | **Enrichment (optional)** | Retrieved entities | KG aliases, definitions |
| **8ï¸âƒ£** | **Answer Synthesis (rag.qa)** | Question + context | Concise answer with citations |

---

### ğŸŒ Data Modalities

| Data Type | Storage | Example |
|:-----------|:--------|:--------|
| ğŸ§± **Entity** | GraphDB | `<https://kg.mcp.ai/id/token/bitcoin>` â†’ `rdf:type crypto:Token` |
| ğŸ“œ **Chunk** | Chroma | â€œBitcoin is a peer-to-peer electronic cash systemâ€¦â€ |
| ğŸ§© **Embedding** | Chroma / Ollama | 768-dim `nomic-embed-text` vector |
| ğŸ§® **Provenance** | Metadata | `doc_id`, `chunk_id`, `page`, `entity_ids[]` |
| ğŸ’¬ **Answer** | MCP JSON | `{ "answer": "...", "citations": [...] }` |

---

### ğŸ§± Core MCP Tools

| Server | Tool | Description |
|:--------|:------|:-------------|
| ğŸ§© **RAG** | `rag.search` | Semantic search over chunks |
| | `rag.embed_and_index` | Add new labeled chunks to index |
| | `rag.reindex` | Rebuild from outputs directory |
| | `rag.delete` | Delete by IDs or filters |
| | `rag.qa` | Question answering with LLM synthesis |
| | `rag.health` | Diagnostics and store info |
| ğŸ§  **KG** | `sparql_query` / `sparql_update` | Execute SPARQL against GraphDB |
| | `push_labels` / `validate_labels` | Add or validate KG entries |
| | `list_documents`, `get_chunk` | Retrieve document metadata |
| | `kg.health` | Check GraphDB repository status |

---

## 4ï¸âƒ£ âš™ï¸ Installation & Setup

Set up your local **GraphRAG MCP environment** in just a few steps!  
This stack runs fully offline and integrates seamlessly with **Ollama**, **GraphDB**, and **Chroma**.

---

### ğŸ§¾ Prerequisites

| Requirement | Description | Example |
|:-------------|:-------------|:----------|
| ğŸ **Python** | Version **3.11+** recommended | `python --version` â†’ `Python 3.11.8` |
| ğŸ§  **Ollama** | Local LLM runtime (for inference + embeddings) | `ollama pull llama3.1:latest` |
| ğŸ§© **GraphDB Desktop 11+** | Local Knowledge Graph database | runs at `http://localhost:7200` |
| ğŸ’¾ **ChromaDB** | Vector store for embeddings | auto-initialized under `.chroma/` |
| ğŸ§° **FastMCP** | Multi-Component Platform runtime (2.x) | installed via `pip` |

---

### ğŸ§± Folder Layout (simplified)

| Folder | Purpose | Example Contents |
|:--------|:----------|:----------------|
| `src/` | Core codebase | `pipeline.py`, `mcp/`, `kg/`, `rag/` |
| `outputs/run_simple/` | Generated outputs | labeled chunks, reports, embeddings |
| `.chroma/` | Chroma persistent vector store | `chroma.sqlite3`, `index/` |
| `.env` | Environment configuration | Ollama, GraphDB, Chroma settings |
| `tests/` | Offline unit tests | `test_rag_qa.py`, `test_kg_server.py` |

---

### ğŸ§° Step-by-Step Setup

#### ğŸª„ 1ï¸âƒ£ Clone & Create Virtual Environment

```bash
git clone <[text](https://github.com/Swissbit92/GraphDB_Desktop.git)>
```

#### âš¡ 2ï¸âƒ£ Activate Environment

| OS | Command |
|:---|:---------|
| ğŸªŸ **Windows (PowerShell)** | `.venv\Scripts\activate` |
| ğŸ§ **Linux / macOS** | `source .venv/bin/activate` |

#### ğŸ“¦ 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

#### âš™ï¸ 4ï¸âƒ£ Verify Installation

```bash
python -m src.mcp.rag_server --list-tools
python -m src.mcp.kg_server --list-tools
```

âœ… You should see tools like **`rag.qa`**, **`rag.search`**, and **`kg.health`**.

---

### ğŸ§  Optional: Preload Ollama Models

| Model | Purpose | Pull Command |
|:-------|:----------|:--------------|
| ğŸ¦™ **llama3.1:latest** | Default reasoning + summarization model | `ollama pull llama3.1:latest` |
| ğŸ§© **nomic-embed-text** | Embedding model for RAG vectorization | `ollama pull nomic-embed-text` |
| ğŸ¤– **qwen2.5:14b-instruct** | Larger model for complex QA tasks | `ollama pull qwen2.5:14b-instruct` |

---

### ğŸ” Quick Sanity Check

Run a quick health diagnostic to ensure everything is configured correctly:

```bash
pytest -q
python -m src.mcp.rag_server --run-tool rag.health
python -m src.mcp.kg_server --run-tool kg.health
```

If both return âœ… **OK**, youâ€™re ready to run the pipeline and start querying your **Knowledge Graph + RAG** system!

---

## 5ï¸âƒ£ ğŸ§ª How to Use & Test

### ğŸ“¥ Ingest Whitepapers & Build the Index

```bash
# Place your PDFs under .\whitepapers\ then run:
python -m src.pipeline --input ".\whitepapers\*.pdf"
```

âœ… Outputs:

- Labeled JSONL â†’ `outputs\run_simple\labels\`
- Chroma index  â†’ `.chroma\`
- (If enabled) Entities pushed to GraphDB repository `mcp_kg`

---

### ğŸ–§ Start the MCP Servers (RAG + KG)

```bash
# Terminal A
python -m src.mcp.rag_server
```

```bash
# Terminal B
python -m src.mcp.kg_server
```

ğŸ’¡ Tip: In another PowerShell window, confirm the tools are available:

```bash
python -m src.mcp.rag_server --list-tools
python -m src.mcp.kg_server --list-tools
```

---

### ğŸ” Quick Retrieval Check (RAG)

```bash
# Example: semantic search for "peer-to-peer electronic cash"
python -m src.mcp.rag_server --run-tool rag.search --input '{ "text": "peer-to-peer electronic cash", "k": 3 }'
```

You should see matching chunks with `doc_id`, `chunk_id`, and distances.

---

### â“ Ask Questions with Citations (rag.qa)

```bash
# Fully offline (deterministic mock answer)
python -m src.mcp.rag_server --run-tool rag.qa --input '{ "question": "What problem does Bitcoin aim to solve?", "k": 5, "kg_enrich": true, "use_mock_llm": true }'
```

â¡ï¸ Returns:

- `answer`: concise response (mock or LLM)
- `citations`: `[ {doc_id, chunk_id, entity_ids, text} ]`
- `took_ms`, `model_used`

Switch to real LLM synthesis by omitting `use_mock_llm` (requires Ollama running).

---

### ğŸ§  Optional: Entity-Filtered QA

```bash
python -m src.mcp.rag_server --run-tool rag.qa --input '{ "question": "How does proof-of-work secure the network?", "entity_ids": ["https://kg.mcp.ai/id/token/bitcoin"], "k": 5, "kg_enrich": true, "use_mock_llm": true }'
```

This restricts retrieval to chunks tagged with the specified KG entity(ies).

---

### ğŸ§ª Run the Test Suite

```bash
pytest -q
```

Key tests (all offline):

- `tests\test_rag_qa.py`: verifies retrieval normalization and mock LLM mode  
- `tests\test_kg_server.py`: checks KG connectivity (skips if GraphDB not running)

---

### ğŸ©º Health Checks

```bash
python -m src.mcp.rag_server --run-tool rag.health
python -m src.mcp.kg_server --run-tool kg.health
```

Expect collection info, document counts, and OK status.

---

### ğŸ§© MCP Coordinator / UI Hookup (Optional)

Ensure your `mcp.json` references the running servers:

```json
{
  "mcpServers": {
    "rag": { "command": "python", "args": ["-m", "src.mcp.rag_server"] },
    "kg":  { "command": "python", "args": ["-m", "src.mcp.kg_server"] }
  }
}
```

Then connect via your MCP Coordinator or Streamlit app to interactively call `rag.qa` and `kg.*` tools.

---

## ğŸ™ Closing Words

GraphRAG MCP is part of the broader **Eeva AI** ecosystem â€” an open, modular framework for intelligent crypto research and strategy generation.  
This project wouldnâ€™t exist without the incredible open-source community that continues to push the boundaries of local AI and knowledge engineering.

If you find this useful:

- â­ **Star the repository** to support ongoing development  
- ğŸ§© **Contribute** improvements or new MCP modules  
- ğŸ§  **Explore** integrations with other MCPs (Brave API, MongoDB, Telegram, etc.)  
- ğŸ’¬ **Share feedback** â€” every suggestion helps make the system smarter, faster, and more reliable

---

> _â€œKnowledge is only powerful when itâ€™s connected.â€_  
> â€” __Eeva AI Research__

Thank you for being part of the open-source journey. ğŸš€

---
